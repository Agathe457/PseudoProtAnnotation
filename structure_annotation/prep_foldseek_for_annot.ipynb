{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d86dee8a-81b5-4498-a101-b0a4098d638c",
   "metadata": {},
   "source": [
    "### Annotation from predicted structures \n",
    "\n",
    "After preparing files for structures prediction of the cluster representatives on the HPC for running FoldSeek against PDB, AF50M and Phold db, moved on to annotation of structures\n",
    "\n",
    "- PDB database version: latest update from 2024-11-29 - update on staging to more recent\n",
    "- AlphaFold database version: latest Alphafold v2.3.2 march 2023 - same as current one available on staging\n",
    "-   Phold database version: v0.2.0 2024-07-13\n",
    "\n",
    "Steps followed:  \n",
    "1. Select the set of the 1495 cluster rep - proteins\n",
    "2. Prepare necessary files for predicting protein structures of these proteins on HPC.\n",
    "3. Predict protein structures of these proteins. \n",
    "4. Prepare necessary files for analysing protein structures of these proteins on HPC.\n",
    "5. Investigate the new function based on structures of each cluster.\n",
    "\n",
    "\n",
    "Taking all template files:  submit_vibfold.py, VIBFold.py, VIBFold_adapted_functions.py and submit.sh stored in ../templates : template job inputs and scripts to be modified:\n",
    "\n",
    "in folder structure_prediction:\n",
    "in each folder batch_XX with XX a batch number between 0 and 20:\n",
    "in folder fastas:\n",
    "    - PROTEIN.fasta: fasta file for every hypothetical protein selected for structure prediction (so a cluster representative)\n",
    "- submit_vibfold_PROTEIN.py: python script to submit VIBFold job for specific protein\n",
    "- submit.sh, VIBFold.py, VIBFold_adapted_functions.py: python/bash scripts to submit and run VIBFold jobs\n",
    "- VIBFOLD_PROTEIN.eXXXXX and VIBFOLD_PROTEIN.oXXXXX: error and output files for VIBFold jobs (generated after run on HPC)\n",
    "- in folder results (generated after run on HPC):\n",
    "in folder PROTEIN:\n",
    "5 unrelaxed .pdb protein structures (PROTEIN_unrelaxed_rank_X_model_X_ptmx_1.pdb) and 1 relaxed .pdb protein structure (PROTEIN_relaxed.pdb): AlphaFold predicted structures3 .png files: 2 for PAE (one for all models - PROTEIN_PAE.png, one for the best model - best_PAE.png) and 1 for pLDDT/coverage (PROTEIN_coverage_lddt.png): quality metrics AlphaFold predicted structures depicted visually1 PROTEIN_info.log file1 PROTEIN_msa_ids.txt: .txt file containing all sequence identifiers of the sequences used in the MSA. Every line is a new identifier.query.fasta: input sequence used for AlphaFold run1 PROTEIN_rank_1_model_X_ptmx_1.json: json file with PAE scores of the best AlphaFold prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f208e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "import sys\n",
    "import pandas as pd\n",
    "import Bio\n",
    "from Bio.PDB import *\n",
    "from json.decoder import JSONDecodeError\n",
    "from more_itertools import collapse\n",
    "import more_itertools as mit\n",
    "from json import JSONDecodeError\n",
    "import re\n",
    "from more_itertools import chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ec196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching sequences of clust representatives\n",
    "merged_df = pd.read_csv('for_protein_fasta.tsv', sep='\\t')\n",
    "os.makedirs('fastas', exist_ok=True)\n",
    "for index, row in merged_df.iterrows():\n",
    "    fasta_filename = f\"fastas/{row['id']}.fasta\"\n",
    "    with open(fasta_filename, 'w') as fasta_file:\n",
    "        fasta_file.write(f\">{row['id']}\\n{row['sequence']}\\n\")\n",
    "\n",
    "print(\"FASTA files have been created in the 'fastas' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1632c133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear reference to different directories\n",
    "pipeline_search_dir = os.getcwd()\n",
    "master_dir = os.path.abspath(os.path.join(pipeline_search_dir, os.pardir, os.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ae76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(os.path.join(pipeline_search_dir, \"c_structure_annotation\", \"structure_prediction\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7232ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to adapt job scripts starting from an adapted version of Jasper Zuallaert's submit_vibfold.py\n",
    "def adapt_vibfold_jobscript(template, infile, outfile, length):\n",
    "    # adapt runtime job based on length protein\n",
    "    if length < 301:\n",
    "        time = \"1:30:00\"\n",
    "    elif length > 300 and length < 501:\n",
    "        time = \"3:30:00\"\n",
    "    else:\n",
    "        time = \"5:00:00\"\n",
    "    #read in data\n",
    "    with open(template, \"r\") as file:\n",
    "        filedata = file.read()\n",
    "    #change script\n",
    "    filedata = filedata.replace(\"inputfasta\", \"fastas/{0}\".format(str(infile)))\n",
    "    filedata = filedata.replace(\"walltime=48:00:00\", \"walltime={0}\".format(time))\n",
    "    #write new job script\n",
    "    with open(outfile, \"w\") as file:\n",
    "        file.write(filedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b8c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing files to submit to HPC\n",
    "template = os.path.join(master_dir, \"templates\", \"submit_vibfold.py\")\n",
    "\n",
    "data_path = 'for_protein_fasta.tsv'\n",
    "data = pd.read_csv(data_path, sep='\\t')\n",
    "batches = chunked(data['id'].tolist(), 500)\n",
    "\n",
    "for index_batch, batch in enumerate(batches):\n",
    "    batch_dir = os.path.join(pipeline_search_dir, \"c_structure_annotation\", \"structure_prediction\", f\"batch_{index_batch}\")\n",
    "    fasta_dir = os.path.join(batch_dir, \"fastas\")\n",
    "    results_dir = os.path.join(batch_dir, \"results\")\n",
    "\n",
    "    os.makedirs(fasta_dir, exist_ok=True)\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    shutil.copyfile(os.path.join(master_dir, \"templates\", \"VIBFold.py\"), os.path.join(batch_dir, \"VIBFold.py\"))\n",
    "    shutil.copyfile(os.path.join(master_dir, \"templates\", \"VIBFold_adapted_functions.py\"), os.path.join(batch_dir, \"VIBFold_adapted_functions.py\"))\n",
    "    shutil.copyfile(os.path.join(master_dir, \"templates\", \"submit.sh\"), os.path.join(batch_dir, \"submit.sh\"))\n",
    "\n",
    "    batch_data = data[data['id'].isin(batch)]\n",
    "\n",
    "    for index_prot, row in batch_data.iterrows():\n",
    "        prot_id = row['id']  \n",
    "        sequence = row['sequence']\n",
    "        length = row.get('length', 0)  \n",
    "\n",
    "        fasta_file_path = os.path.join(fasta_dir, f\"{prot_id}.fasta\")\n",
    "        with open(fasta_file_path, \"w\") as fasta_file:\n",
    "            fasta_file.write(f\">{prot_id}\\n{sequence}\\n\")\n",
    "\n",
    "        # Prepare and write the job script\n",
    "        submit_path = os.path.join(batch_dir, f\"submit_vibfold_{prot_id}.py\")\n",
    "        template = os.path.join(master_dir, \"templates\", \"submit_vibfold.py\")\n",
    "        adapt_vibfold_jobscript(template, f\"{prot_id}.fasta\", submit_path, length)\n",
    "\n",
    "        with open(os.path.join(batch_dir, \"submit.sh\"), \"a+\") as submit_file:\n",
    "            submit_file_name = f\"submit_vibfold_{prot_id}.py\"\n",
    "            submit_file.write(f\"python {submit_file_name} \\n\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "735b6384",
   "metadata": {},
   "source": [
    "After running FoldSeek, must prepare files for analysis and annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8a812",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'for_protein_fasta.tsv'\n",
    "data = pd.read_csv(data_path, sep='\\t')  \n",
    "filtered_clustered_reps_int = data['id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b21d9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"c_structure_annotation/structure_prediction\"\n",
    "\n",
    "directory_names = []\n",
    "\n",
    "for batch in [\"batch_0\", \"batch_1\", \"batch_2\"]:\n",
    "    results_path = os.path.join(base_path, batch, \"results\")\n",
    "    \n",
    "    # Check each directory within the results folder\n",
    "    for entry in os.listdir(results_path):\n",
    "        full_path = os.path.join(results_path, entry)\n",
    "        if os.path.isdir(full_path):\n",
    "            directory_names.append(entry)\n",
    "\n",
    "#print(directory_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609724c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "proteins_structure_success = directory_names.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be84565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copying of relaxed structures\n",
    "comparison_dir = os.path.join(pipeline_search_dir, \"c_structure_annotation\", \"structure_comparison\")\n",
    "\n",
    "os.makedirs(comparison_dir, exist_ok=True)\n",
    "\n",
    "for protein in directory_names:\n",
    "    batch_path = os.path.join(pipeline_search_dir, \"c_structure_annotation\", \"structure_prediction\", \"batch_0\" , \"results\")\n",
    "    relaxed_path = os.path.join(batch_path, protein, f\"{protein}_relaxed.pdb\")\n",
    "    if os.path.exists(relaxed_path):\n",
    "        shutil.copyfile(relaxed_path, os.path.join(comparison_dir, f\"{protein}_relaxed.pdb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae75270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating into batches\n",
    "def batched(iterable, n):\n",
    "    \"\"\"Helper function to divide an iterable into batches of size n.\"\"\"\n",
    "    length = len(iterable)\n",
    "    for ndx in range(0, length, n):\n",
    "        yield iterable[ndx:min(ndx + n, length)]\n",
    "\n",
    "base_dir = \"c_structure_annotation/structure_comparison\"\n",
    "files = [f for f in os.listdir(base_dir) if os.path.isfile(os.path.join(base_dir, f))]\n",
    "for index_batch, batch in enumerate(batched(files, 300)):\n",
    "    batch_dir = os.path.join(base_dir, f\"batch_{index_batch}\")\n",
    "    os.makedirs(batch_dir, exist_ok=True)\n",
    "\n",
    "    for filename in batch:\n",
    "        source_path = os.path.join(base_dir, filename)\n",
    "        destination_path = os.path.join(batch_dir, filename)\n",
    "        shutil.move(source_path, destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bc680b1-d3df-4411-b21f-ab6c2f2db843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: more-itertools in /vsc-hard-mounts/leuven-apps/rocky8/icelake/2021a/software/Python/3.9.5-GCCcore-10.3.0/lib/python3.9/site-packages (8.7.0)\n",
      "\u001b[33mWARNING: Error parsing dependencies of ipykernel: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    matplotlib-inline (<0.2.0appnope,>=0.1.0) ; platform_system == \"Darwin\"\n",
      "                      ~~~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install more-itertools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be4c14ae-37c7-43e7-8f53-c993a335f4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: biopython in /vsc-hard-mounts/leuven-user/351/vsc35101/.local/lib/python3.9/site-packages (1.84)\n",
      "Requirement already satisfied: numpy in /vsc-hard-mounts/leuven-user/351/vsc35101/.local/lib/python3.9/site-packages (from biopython) (2.0.1)\n",
      "\u001b[33mWARNING: Error parsing dependencies of ipykernel: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    matplotlib-inline (<0.2.0appnope,>=0.1.0) ; platform_system == \"Darwin\"\n",
      "                      ~~~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install biopython \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffaaf4d0-bd14-493e-93f9-07f83f48b6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# This file may be used to create an environment using:\n",
      "# $ conda create --name <env> --file <this file>\n",
      "# platform: linux-64\n",
      "# created-by: conda 24.11.0\n",
      "@EXPLICIT\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/_libgcc_mutex-0.1-main.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/ca-certificates-2024.11.26-h06a4308_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/ld_impl_linux-64-2.38-h1181459_1.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/libstdcxx-ng-11.2.0-h1234567_1.conda\n",
      "https://repo.anaconda.com/pkgs/main/noarch/pybind11-abi-5-hd3eb1b0_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/noarch/tzdata-2024a-h04d1e81_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/libgomp-11.2.0-h1234567_1.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/_openmp_mutex-5.1-1_gnu.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/libgcc-ng-11.2.0-h1234567_1.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/bzip2-1.0.8-h5eee18b_6.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/c-ares-1.19.1-h5eee18b_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/expat-2.6.2-h6a678d5_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/fmt-9.1.0-hdb19cb5_1.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/icu-73.1-h6a678d5_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/libev-4.33-h7f8727e_1.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/libffi-3.4.4-h6a678d5_1.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/libuuid-1.41.5-h5eee18b_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/lz4-c-1.9.4-h6a678d5_1.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/ncurses-6.4-h6a678d5_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/openssl-3.0.15-h5eee18b_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/reproc-14.2.4-h6a678d5_2.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/xz-5.4.6-h5eee18b_1.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/yaml-cpp-0.8.0-h6a678d5_1.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/zlib-1.2.13-h5eee18b_1.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/libedit-3.1.20230828-h5eee18b_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/libnghttp2-1.57.0-h2d74bed_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/libssh2-1.11.0-h251f7ec_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/libxml2-2.13.1-hfdd30dd_2.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/pcre2-10.42-hebb0a14_1.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/readline-8.2-h5eee18b_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/reproc-cpp-14.2.4-h6a678d5_2.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/tk-8.6.14-h39e8969_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/zstd-1.5.5-hc292b87_2.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/krb5-1.20.1-h143b758_1.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/libarchive-3.6.2-hfab0078_4.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/libsolv-0.7.24-he621ea3_1.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/sqlite-3.45.3-h5eee18b_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/libcurl-8.7.1-h251f7ec_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/python-3.12.4-h5148396_1.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/libmamba-1.5.8-hfe524e5_2.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/menuinst-2.1.2-py312h06a4308_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/anaconda-anon-usage-0.4.4-py312hfc0e8ea_100.conda\n",
      "https://repo.anaconda.com/pkgs/main/noarch/archspec-0.2.3-pyhd3eb1b0_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/boltons-23.0.0-py312h06a4308_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/brotli-python-1.0.9-py312h6a678d5_8.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/certifi-2024.8.30-py312h06a4308_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/noarch/charset-normalizer-3.3.2-pyhd3eb1b0_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/distro-1.9.0-py312h06a4308_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/frozendict-2.4.2-py312h06a4308_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/idna-3.7-py312h06a4308_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/noarch/jsonpointer-2.1-pyhd3eb1b0_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/libmambapy-1.5.8-py312h2dafd23_2.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/packaging-24.1-py312h06a4308_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/platformdirs-3.10.0-py312h06a4308_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/pluggy-1.0.0-py312h06a4308_1.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/pycosat-0.6.6-py312h5eee18b_1.conda\n",
      "https://repo.anaconda.com/pkgs/main/noarch/pycparser-2.21-pyhd3eb1b0_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/pysocks-1.7.1-py312h06a4308_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/ruamel.yaml-0.17.21-py312h5eee18b_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/setuptools-72.1.0-py312h06a4308_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/tqdm-4.66.4-py312he106c6f_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/truststore-0.8.0-py312h06a4308_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/wheel-0.43.0-py312h06a4308_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/cffi-1.16.0-py312h5eee18b_1.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/jsonpatch-1.33-py312h06a4308_1.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/pip-24.2-py312h06a4308_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/urllib3-2.2.2-py312h06a4308_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/cryptography-42.0.5-py312hdda0065_1.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/requests-2.32.3-py312h06a4308_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/zstandard-0.22.0-py312h2c38b39_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/conda-content-trust-0.2.0-py312h06a4308_1.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/conda-package-streaming-0.10.0-py312h06a4308_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/conda-package-handling-2.3.0-py312h06a4308_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/linux-64/conda-24.11.0-py312h06a4308_0.conda\n",
      "https://repo.anaconda.com/pkgs/main/noarch/conda-libmamba-solver-24.7.0-pyhd3eb1b0_0.conda\n"
     ]
    }
   ],
   "source": [
    "!conda list --explicit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4da36fa6-d332-442b-9eab-f1576a9c47e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for requests\n",
    "sess = requests.Session()\n",
    "adapter = requests.adapters.HTTPAdapter(max_retries = 10)\n",
    "sess.mount(\"https://\", adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7555b61-82de-483e-b0fa-1586bc6fbf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear reference to different directories\n",
    "pipeline_search_dir = os.getcwd()\n",
    "master_dir = os.path.abspath(os.path.join(pipeline_search_dir, os.pardir, os.pardir))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4eb79748-68e3-47a3-9846-2e4a18e17cec",
   "metadata": {},
   "source": [
    "Processing the FoldSeek result files\n",
    "\n",
    "Using the created scripts, protein structures were compared to the PDB, Phold and AlphaFold database on the HPC. Results were downloaded, and all files created in the process are stored in  subdirectory 'structure_comparison'. While the comparison of our structure has been done, the output files are not very interpretable: matched proteins are described based on UniProt ID or PDB ID or Phrog ID.\n",
    "\n",
    "Hence, we have to process the output to link protein names to these identifiers. In addition, we will also apply some quality filters to our hits. Briefly, we will filter based on:\n",
    "- probability of our protein and its match to belong to the same SCOP class: > 50%\n",
    "- quality of the alignment: alignment lDDT > 50% (based on visual inspection of alignments)\n",
    "- likelihood of the hit: E-value cutoff E-3 (FoldSeek default)\n",
    "- quality of the aligned region of input protein: average plDDT of aligned region > 70 (standard AlphaFold cut-off)\n",
    "\n",
    "In order to do so, we write functions to extract these features, and to match the PDB/UniProt/Phrog information to protein names. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef87b4cf-1562-426e-b614-d86338c1135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"c_structure_annotation/structure_prediction\"\n",
    "directory_names = []\n",
    "\n",
    "# Loop through each batch folder\n",
    "for batch in [\"batch_0\", \"batch_1\", \"batch_2\"]:\n",
    "    results_path = os.path.join(base_path, batch, \"results\")\n",
    "    for entry in os.listdir(results_path):\n",
    "        full_path = os.path.join(results_path, entry)\n",
    "        if os.path.isdir(full_path):\n",
    "            directory_names.append(entry)\n",
    "\n",
    "# check cluster rep which structure has been successfully predicted\n",
    "#print(directory_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48e5f73d-5d07-444e-af91-f84ca417e186",
   "metadata": {},
   "source": [
    "bugs for following proteins: so remove\n",
    "CAR312481\n",
    "Station158_DCM_ALL_assembly_NODE_1847_length_35185_cov_4285255_36\n",
    "biochar_58_26\n",
    "biochar_267_6\n",
    "biochar_464_13\n",
    "biochar_638_45\n",
    "biochar_884_13\n",
    "biochar_2218_16\n",
    "biochar_2253_1\n",
    "biochar_2974_7\n",
    "biochar_3169_9\n",
    "biochar_3173_15\n",
    "biochar_3481_12\n",
    "biochar_3496_7\n",
    "biochar_3567_15\n",
    "biochar_3928_1\n",
    "biochar_4120_8\n",
    "biochar_4122_1\n",
    "biochar_4254_1\n",
    "biochar_4411_4\n",
    "biochar_4538_2\n",
    "biochar_4592_12\n",
    "biochar_4597_10\n",
    "biochar_4628_1\n",
    "biochar_4628_4\n",
    "biochar_4928_3\n",
    "biochar_4936_1\n",
    "biochar_4991_12\n",
    "biochar_5172_12\n",
    "biochar_5399_2\n",
    "biochar_5439_1\n",
    "biochar_5571_1\n",
    "biochar_5862_1\n",
    "biochar_5897_23\n",
    "biochar_5984_13\n",
    "biochar_6075_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad071e39-6357-4ec9-b739-caa7ec1419d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of all protein IDs for which structure prediction succeeded \n",
    "    # this is all the proteins for which structure prediction was started\n",
    "proteins_structure_success = directory_names.copy()\n",
    "    # except the NaN cluster representative and the buggy ones\n",
    "#proteins_structure_success.remove('CAR312481')\n",
    "\n",
    "bug_ids = ['CAR312481','Station158_DCM_ALL_assembly_NODE_1847_length_35185_cov_4285255_36',\n",
    "'biochar_58_26','biochar_267_6','biochar_464_13','biochar_638_45','biochar_884_13',\n",
    "'biochar_2218_16','biochar_2253_1','biochar_2974_7','biochar_3169_9','biochar_3173_15','biochar_3481_12',\n",
    "'biochar_3496_7','biochar_3567_15','biochar_3928_1','biochar_4120_8','biochar_4122_1','biochar_4254_1',\n",
    "'biochar_4411_4','biochar_4538_2','biochar_4592_12','biochar_4597_10','biochar_4628_1','biochar_4628_4',\n",
    "'biochar_4928_3','biochar_4936_1','biochar_4991_12','biochar_5172_12', 'biochar_5399_2', 'biochar_5439_1',\n",
    "'biochar_5571_1','biochar_5862_1','biochar_5897_23','biochar_5984_13', 'biochar_6075_12']\n",
    "\n",
    "for id in bug_ids:\n",
    "    proteins_structure_success.remove(id)\n",
    "# creating a dictionary storing whether relaxed or best structure should be used\n",
    "    # list of the IDs (based on output checks)\n",
    "\n",
    "#now add list where relaxation failed and using best is better\n",
    "# creating a dictionary storing whether relaxed or best structure should be used\n",
    "    # list of the IDs (based on output checks)\n",
    "relaxation_failed_ids = ['']\n",
    "    \n",
    "    # creating dictionary\n",
    "relax_best_dict = {}\n",
    "for key in proteins_structure_success:\n",
    "    if key in proteins_structure_success:\n",
    "        relax_best_dict[key] = \"relax\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29501132-ebe6-40df-9e6f-825ea403e048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using `chunked` to create chunks of size 500 from `filtered_clustered_reps_int`\n",
    "for index_chunk, chunk in enumerate(mit.chunked(proteins_structure_success, 500)):\n",
    "    batch_path = os.path.join(pipeline_search_dir, \"c_structure_annotation\", \"structure_prediction\", f\"batch_{index_chunk}\")\n",
    "    \n",
    "    # Ensure the batch_path directory exists\n",
    "    if not os.path.exists(batch_path):\n",
    "        os.makedirs(batch_path)\n",
    "    \n",
    "    # Iterate through each protein in the chunk\n",
    "    for protein in chunk:\n",
    "        # Path for proteins with relaxed structure\n",
    "        relaxed_path = os.path.join(batch_path, \"results\", f\"{protein}\", f\"{protein}_relaxed.pdb\")\n",
    "        \n",
    "        # Check if the relaxed structure file exists before copying\n",
    "        if os.path.exists(relaxed_path):\n",
    "            destination_path = os.path.join(pipeline_search_dir, \"c_structure_annotation\", \"structure_comp\", f\"{protein}_relaxed.pdb\")\n",
    "            \n",
    "            # Ensure the destination directory exists\n",
    "            destination_dir = os.path.dirname(destination_path)\n",
    "            if not os.path.exists(destination_dir):\n",
    "                os.makedirs(destination_dir)\n",
    "            \n",
    "            shutil.copyfile(relaxed_path, destination_path)\n",
    "        else:\n",
    "            print(f\" {protein}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0263f1ab-61a8-458a-a067-5ca13eeb3b58",
   "metadata": {},
   "source": [
    "As there were some issues with file not found when running the code the first time, added checks to ensure every relaxed file is successfully added and reran problematic ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a7516ef-1040-42c1-8174-caa2fd25f7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering foldseek results\n",
    "# define a PDB parser\n",
    "p = PDBParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "810a7e9f-54bc-43d7-a37a-3f7cfc7364dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for calculating average pLDDT of aligned region on results\n",
    "def get_pLDDT(structure, start, stop):\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            count_residue = 0\n",
    "            pLDDT_chain = 0\n",
    "            for residue in chain:\n",
    "                if residue.id[1] in range(start,stop+1):\n",
    "                    count_residue += 1\n",
    "                    count_atom = 0\n",
    "                    pLDDT_atom = 0\n",
    "                    for atom in residue:\n",
    "                        count_atom += 1\n",
    "                        pLDDT_atom += atom.get_bfactor()\n",
    "                    pLDDT_res = round(pLDDT_atom/count_atom,2)\n",
    "                    pLDDT_chain += pLDDT_res\n",
    "            pLDDT_range = round(pLDDT_chain/count_residue,2)\n",
    "    return pLDDT_range "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde37877-9f5d-4e1f-8608-80b76619e24e",
   "metadata": {},
   "source": [
    "in structure comp, we have all the proteins for which there was successful prediction (relaxed file exists). we now want to create a few folders/batches to seperate these files for easier parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93c4361-06ee-4f82-8229-ed8f52ea7de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_DIR = \"c_structure_annotation/structure_comp\"  \n",
    "MAX_FILES_PER_FOLDER = 300\n",
    "\n",
    "# Get all files in the directory\n",
    "files = [f for f in os.listdir(SOURCE_DIR) if os.path.isfile(os.path.join(SOURCE_DIR, f))]\n",
    "\n",
    "# Create and move files into numbered folders in chunks of 300\n",
    "for i in range(0, len(files), MAX_FILES_PER_FOLDER):\n",
    "    folder_name = os.path.join(SOURCE_DIR, f\"batch_{i // MAX_FILES_PER_FOLDER + 1}\")\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    \n",
    "    for file in files[i:i + MAX_FILES_PER_FOLDER]:\n",
    "        shutil.move(os.path.join(SOURCE_DIR, file), os.path.join(folder_name, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bc45c2-6104-437c-9125-049c87dd4791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched(iterable, n):\n",
    "    \"\"\"Helper function to divide an iterable into batches of size n.\"\"\"\n",
    "    length = len(iterable)\n",
    "    for ndx in range(0, length, n):\n",
    "        yield iterable[ndx:min(ndx + n, length)]\n",
    "\n",
    "pipeline_search_dir = \"c_structure_annotation/structure_comp\"\n",
    "master_dir = \"c_structure_annotation\"  \n",
    "\n",
    "batch_dirs = [name for name in os.listdir(pipeline_search_dir) if os.path.isdir(os.path.join(pipeline_search_dir, name)) and name.startswith(\"batch_\")]\n",
    "\n",
    "for batch_dir_name in batch_dirs:\n",
    "    batch_dir = os.path.join(pipeline_search_dir, batch_dir_name)\n",
    "\n",
    "    shutil.copyfile(os.path.join(master_dir, \"data_foldseek_5.csv\"),\n",
    "                    os.path.join(batch_dir, \"data_foldseek_5.csv\"))\n",
    "\n",
    "    files = [file for file in os.listdir(batch_dir) if os.path.isfile(os.path.join(batch_dir, file)) and file != \"data_foldseek_5.csv\"]\n",
    "    with open(os.path.join(batch_dir, \"data_foldseek_5.csv\"), \"a+\") as data_file:\n",
    "        for sub_batch in list(batched(files, 5)):\n",
    "            line = \",\".join([f\"{filename},{filename.replace('.pdb', '_aln_pdb.txt')},{filename.replace('.pdb', '_aln_af50m.txt')}, {filename.replace('.pdb', '_phold.txt')}\" for filename in sub_batch])\n",
    "            data_file.write(line + \"\\n\")\n",
    "            \n",
    "\n",
    "    shutil.copyfile(os.path.join(master_dir, \"script_foldseek_5.slurm\"),\n",
    "                    os.path.join(batch_dir, \"script_foldseek_5.slurm\"))\n",
    "\n",
    "print(\"Setup of job scripts and data files for each batch completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cfd51f-1611-4a8b-bd61-07fe662cf06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_DIR = \"c_structure_annotation/structure_comp\" \n",
    "\n",
    "def batch(iterable, size):\n",
    "    \"\"\"Yield successive n-sized chunks from iterable.\"\"\"\n",
    "    iterator = iter(iterable)\n",
    "    while chunk := list(islice(iterator, size)):\n",
    "        yield chunk\n",
    "\n",
    "def get_pLDDT(structure, start, stop):\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            count_residue = 0\n",
    "            pLDDT_chain = 0\n",
    "            for residue in chain:\n",
    "                if residue.id[1] in range(start,stop+1):\n",
    "                    count_residue += 1\n",
    "                    count_atom = 0\n",
    "                    pLDDT_atom = 0\n",
    "                    for atom in residue:\n",
    "                        count_atom += 1\n",
    "                        pLDDT_atom += atom.get_bfactor()\n",
    "                    pLDDT_res = round(pLDDT_atom/count_atom,2)\n",
    "                    pLDDT_chain += pLDDT_res\n",
    "            pLDDT_range = round(pLDDT_chain/count_residue,2)\n",
    "    return pLDDT_range \n",
    "\n",
    "# Get all existing batch folders\n",
    "batch_folders = sorted([f for f in os.listdir(SOURCE_DIR) if os.path.isdir(os.path.join(SOURCE_DIR, f)) and f.startswith(\"batch_\")])\n",
    "\n",
    "for batch_folder in batch_folders:\n",
    "    batch_dir = os.path.join(SOURCE_DIR, batch_folder)\n",
    "    files = [f for f in os.listdir(batch_dir) if os.path.isfile(os.path.join(batch_dir, f))]\n",
    "\n",
    "    for file in files:\n",
    "        protein = os.path.splitext(file)[0]  \n",
    "        structure = os.path.join(batch_dir, f\"{protein}_relaxed.pdb\")  \n",
    "        \n",
    "\n",
    "        # Process FoldSeek results against AF50m database\n",
    "        af50m_path = os.path.join(batch_dir, f\"{protein}_aln_af50m.txt\")\n",
    "        # filtering hits against AF50m database\n",
    "            # reading in the data\n",
    "        if os.path.exists(af50m_path):\n",
    "            data_af50m = pd.read_table(af50m_path, header=None, names=[\"query\",\"target\",\"fident\",\"alnlen\",\"mismatch\",\"gapopen\",\"qstart\",\"qend\",\"tstart\",\"tend\",\"evalue\",\"bits\",\"prob\",\"lddt\",\"lddtfull\"])\n",
    "            \n",
    "            # adding AlphaFold quality metrics - pLDDT of aligned region query\n",
    "\n",
    "            # Add pLDDT column\n",
    "            for i in range(len(data_af50m)):\n",
    "                data_af50m.at[i, \"pLDDT_qAln\"] = get_pLDDT(structure, data_af50m.at[i, \"qstart\"], data_af50m.at[i, \"qend\"])\n",
    "\n",
    "            # Apply filtering\n",
    "            data_af50m_filtered = data_af50m[(data_af50m[\"prob\"] > 0.5) & (data_af50m[\"evalue\"] < 1e-3) & (data_af50m[\"lddt\"] >= 0.5) & (data_af50m[\"pLDDT_qAln\"] >= 0.7)]\n",
    "            data_af50m_filtered = data_af50m_filtered.reset_index(drop=True)\n",
    "            data_af50m_filtered.to_csv(os.path.join(batch_dir, f\"{protein}_foldseek_af50m_filtered.csv\"), index=False)\n",
    "\n",
    "        # Process FoldSeek results against PDB database\n",
    "        pdb_path = os.path.join(batch_dir, f\"{protein}_aln_pdb.txt\")\n",
    "        if os.path.exists(pdb_path):\n",
    "            data_pdb = pd.read_table(pdb_path, header=None, names=[\"query\",\"target\",\"fident\",\"alnlen\",\"mismatch\",\"gapopen\",\"qstart\",\"qend\",\"tstart\",\"tend\",\"evalue\",\"bits\",\"prob\",\"lddt\",\"lddtfull\"])\n",
    "            \n",
    "            # Add pLDDT column\n",
    "            for i in range(len(data_pdb)):\n",
    "                data_pdb.at[i, \"pLDDT_qAln\"] = get_pLDDT(structure, data_pdb.at[i, \"qstart\"], data_pdb.at[i, \"qend\"])\n",
    "\n",
    "            # Apply filtering\n",
    "            data_pdb_filtered = data_pdb[(data_pdb[\"prob\"] > 0.5) & (data_pdb[\"evalue\"] < 1e-3) & (data_pdb[\"lddt\"] >= 0.5) & (data_pdb[\"pLDDT_qAln\"] >= 0.7)]\n",
    "            data_pdb_filtered = data_pdb_filtered.reset_index(drop=True)\n",
    "\n",
    "            # Save filtered results\n",
    "            data_pdb_filtered.to_csv(os.path.join(batch_dir, f\"{protein}_foldseek_pdb_filtered.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaedb82e-fb1e-40e1-bfad-b39658090979",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_batch, batch in enumerate(list(batched(relax_best_dict.keys(),300))):\n",
    "    batch_dir = os.path.join(pipeline_search_dir, \"c_structure_annotation\", \"structure_comparison\", f\"batch_{index_batch}\")  \n",
    "    for protein in batch:\n",
    "        # get protein structure from AlphaFold\n",
    "        if relax_best_dict.get(protein) == \"relax\":\n",
    "            structure = p.get_structure(protein, os.path.join(batch_dir, \"{0}_relaxed.pdb\".format(protein)))\n",
    "        if relax_best_dict.get(protein) == \"best\":\n",
    "            structure = p.get_structure(protein, os.path.join(batch_dir, \"{0}_best.pdb\".format(protein)))\n",
    "        # filtering hits against AF50m database\n",
    "            # reading in the data\n",
    "        data_af50m = pd.read_table(os.path.join(batch_dir, \"{0}_aln_af50m.txt\".format(protein)), header=None, names=[\"query\",\"target\",\"fident\",\"alnlen\",\"mismatch\",\"gapopen\",\"qstart\",\"qend\",\"tstart\",\"tend\",\"evalue\",\"bits\",\"prob\",\"lddt\",\"lddtfull\"])\n",
    "            # adding AlphaFold quality metrics - pLDDT of aligned region query\n",
    "        for i in range(0,len(data_af50m)):\n",
    "            data_af50m.at[i,\"pLDDT_qAln\"] = get_pLDDT(structure, data_af50m.at[i,\"qstart\"],data_af50m.at[i,\"qend\"])\n",
    "            # filtering based on FoldSeek parameters & AF model accuracy\n",
    "        data_af50m_filtered = data_af50m\n",
    "        if len(data_af50m_filtered) != 0:\n",
    "            data_af50m_filtered = data_af50m_filtered[(data_af50m_filtered[\"prob\"] > 0.5) & (data_af50m_filtered[\"evalue\"] < 1e-3) & (data_af50m_filtered[\"lddt\"] >= 0.5) & (data_af50m_filtered[\"pLDDT_qAln\"] >= 0.7)]\n",
    "        data_af50m_filtered = data_af50m_filtered.reset_index()\n",
    "            # store filtered FoldSeek results\n",
    "        data_af50m_filtered.to_csv(os.path.join(batch_dir, f\"{protein}_foldseek_af50m_filtered.csv\"), index=False)     \n",
    "        # filtering hits against PDB database\n",
    "            # reading in the data\n",
    "        data_pdb = pd.read_table(os.path.join(batch_dir, \"{0}_aln_pdb.txt\".format(protein)), header=None, names=[\"query\",\"target\",\"fident\",\"alnlen\",\"mismatch\",\"gapopen\",\"qstart\",\"qend\",\"tstart\",\"tend\",\"evalue\",\"bits\",\"prob\",\"lddt\",\"lddtfull\"])\n",
    "            # adding AlphaFold quality metrics - pLDDT of aligned region query\n",
    "        for i in range(0,len(data_pdb)):\n",
    "            data_pdb.at[i,\"pLDDT_qAln\"] = get_pLDDT(structure, data_pdb.at[i,\"qstart\"],data_pdb.at[i,\"qend\"])\n",
    "            # filtering based on FoldSeek parameters & AF model accuracy\n",
    "        data_pdb_filtered = data_pdb\n",
    "        if len(data_pdb_filtered) != 0:\n",
    "            data_pdb_filtered = data_pdb_filtered[(data_pdb_filtered[\"prob\"] > 0.5) & (data_pdb_filtered[\"evalue\"] < 1e-3) & (data_pdb_filtered[\"lddt\"] >= 0.5) & (data_pdb_filtered[\"pLDDT_qAln\"] >= 0.7)]\n",
    "        data_pdb_filtered = data_pdb_filtered.reset_index()\n",
    "            # store filtered FoldSeek results\n",
    "        data_pdb_filtered.to_csv(os.path.join(batch_dir, f\"{protein}_foldseek_pdb_filtered.csv\"), index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "030135ac-8b9f-4d8e-9913-0931bb89d9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pLDDT(structure, start, stop):\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            count_residue = 0\n",
    "            pLDDT_chain = 0\n",
    "            for residue in chain:\n",
    "                if residue.id[1] in range(start,stop+1):\n",
    "                    count_residue += 1\n",
    "                    count_atom = 0\n",
    "                    pLDDT_atom = 0\n",
    "                    for atom in residue:\n",
    "                        count_atom += 1\n",
    "                        pLDDT_atom += atom.get_bfactor()\n",
    "                    pLDDT_res = round(pLDDT_atom/count_atom,2)\n",
    "                    pLDDT_chain += pLDDT_res\n",
    "            pLDDT_range = round(pLDDT_chain/count_residue,2)\n",
    "    return pLDDT_range \n",
    "\n",
    "# Define parent directory where batches are stored\n",
    "pipeline_search_dir = \"c_structure_annotation/structure_comp\"  \n",
    "\n",
    "# Loop through batch directories\n",
    "for index_batch in range(1, 6):  # Adjust range if needed\n",
    "    batch_dir = os.path.join(pipeline_search_dir, f\"batch_{index_batch}\")\n",
    "\n",
    "    # Loop through protein files in the batch directory\n",
    "    for filename in os.listdir(batch_dir):\n",
    "        if filename.endswith(\"_relaxed.pdb\"):  # Ensure processing only PDB files\n",
    "            protein = filename.replace(\"_relaxed.pdb\", \"\")  # Extract protein name\n",
    "\n",
    "            # Load protein structure (since all are \"relax\")\n",
    "            structure = p.get_structure(protein, os.path.join(batch_dir, filename))\n",
    "\n",
    "            # Process FoldSeek results for AF50m database\n",
    "            af50m_file = os.path.join(batch_dir, f\"{protein}_aln_af50m.txt\")\n",
    "            if os.path.exists(af50m_file):\n",
    "                data_af50m = pd.read_table(af50m_file, header=None, names=[\n",
    "                    \"query\", \"target\", \"fident\", \"alnlen\", \"mismatch\", \"gapopen\", \"qstart\", \"qend\",\n",
    "                    \"tstart\", \"tend\", \"evalue\", \"bits\", \"prob\", \"lddt\", \"lddtfull\"\n",
    "                ])\n",
    "                \n",
    "                # Add AlphaFold quality metrics\n",
    "                for i in range(len(data_af50m)):\n",
    "                    data_af50m.at[i, \"pLDDT_qAln\"] = get_pLDDT(structure, data_af50m.at[i, \"qstart\"], data_af50m.at[i, \"qend\"])\n",
    "                \n",
    "                # Apply filtering\n",
    "                data_af50m_filtered = data_af50m[(data_af50m[\"prob\"] > 0.5) & (data_af50m[\"evalue\"] < 1e-3) &\n",
    "                                                 (data_af50m[\"lddt\"] >= 0.5) & (data_af50m[\"pLDDT_qAln\"] >= 0.7)]\n",
    "                \n",
    "                # Save filtered results\n",
    "                data_af50m_filtered.to_csv(os.path.join(batch_dir, f\"{protein}_foldseek_af50m_filtered.csv\"), index=False)\n",
    "\n",
    "            # Process FoldSeek results for PDB database\n",
    "            pdb_file = os.path.join(batch_dir, f\"{protein}_aln_pdb.txt\")\n",
    "            if os.path.exists(pdb_file):\n",
    "                data_pdb = pd.read_table(pdb_file, header=None, names=[\n",
    "                    \"query\", \"target\", \"fident\", \"alnlen\", \"mismatch\", \"gapopen\", \"qstart\", \"qend\",\n",
    "                    \"tstart\", \"tend\", \"evalue\", \"bits\", \"prob\", \"lddt\", \"lddtfull\"\n",
    "                ])\n",
    "                \n",
    "                # Add AlphaFold quality metrics\n",
    "                for i in range(len(data_pdb)):\n",
    "                    data_pdb.at[i, \"pLDDT_qAln\"] = get_pLDDT(structure, data_pdb.at[i, \"qstart\"], data_pdb.at[i, \"qend\"])\n",
    "                \n",
    "                # Apply filtering\n",
    "                data_pdb_filtered = data_pdb[(data_pdb[\"prob\"] > 0.5) & (data_pdb[\"evalue\"] < 1e-3) &\n",
    "                                             (data_pdb[\"lddt\"] >= 0.5) & (data_pdb[\"pLDDT_qAln\"] >= 0.7)]\n",
    "                \n",
    "                # Save filtered results\n",
    "                data_pdb_filtered.to_csv(os.path.join(batch_dir, f\"{protein}_foldseek_pdb_filtered.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b126fbad-ede6-455e-9436-c8cafcfabf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_search_dir = \"c_structure_annotation\"  \n",
    "# Iterate over batch folders from batch_1 to batch_5\n",
    "for index_batch in range(1, 6):  \n",
    "    batch_dir = os.path.join(pipeline_search_dir, \"structure_comp\", f\"batch_{index_batch}\")\n",
    "\n",
    "    # Check all proteins in the batch directory\n",
    "    for filename in os.listdir(batch_dir):\n",
    "        if filename.endswith(\"_relaxed.pdb\"):  \n",
    "            protein = filename.replace(\"_relaxed.pdb\", \"\") \n",
    "\n",
    "            alnfile_af50m = os.path.join(batch_dir, f\"{protein}_relaxed_aln_af50m.txt\")\n",
    "            alnfile_pdb = os.path.join(batch_dir, f\"{protein}_relaxed_aln_pdb.txt\")\n",
    "            alnfile_phold = os.path.join(batch_dir, f\"{protein}_relaxed_phold.txt\")\n",
    "\n",
    "            # Check if all files exist\n",
    "            if not (os.path.isfile(alnfile_af50m) and os.path.isfile(alnfile_pdb)):\n",
    "                #print(f\"Issue finding FoldSeek output files for protein {protein} in batch_{index_batch}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2847fe0d-1d53-467f-a92d-4713c8c7b313",
   "metadata": {},
   "source": [
    "5. Investigate function based on structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6167f577-d88e-4fcf-bb83-6b14e815241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a PDB parser\n",
    "p = PDBParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e9f5a4c-debd-47c2-bfb2-5b90f304af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for calculating average pLDDT of aligned region on FoldSeek results\n",
    "def get_pLDDT(structure, start, stop):\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            count_residue = 0\n",
    "            pLDDT_chain = 0\n",
    "            for residue in chain:\n",
    "                if residue.id[1] in range(start,stop+1):\n",
    "                    count_residue += 1\n",
    "                    count_atom = 0\n",
    "                    pLDDT_atom = 0\n",
    "                    for atom in residue:\n",
    "                        count_atom += 1\n",
    "                        pLDDT_atom += atom.get_bfactor()\n",
    "                    pLDDT_res = round(pLDDT_atom/count_atom,2)\n",
    "                    pLDDT_chain += pLDDT_res\n",
    "            pLDDT_range = round(pLDDT_chain/count_residue,2)\n",
    "    return pLDDT_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df8a7d1b-7df1-4c35-81ea-da3951a0995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_search_dir = \"c_structure_annotation/structure_comp\"  \n",
    "\n",
    "for index_batch in range(1, 6):  \n",
    "    batch_dir = os.path.join(pipeline_search_dir, f\"batch_{index_batch}\")\n",
    "\n",
    "    for filename in os.listdir(batch_dir):\n",
    "        if filename.endswith(\"_relaxed.pdb\"):  \n",
    "            protein = filename.replace(\"_relaxed.pdb\", \"\")  \n",
    "            structure = p.get_structure(protein, os.path.join(batch_dir, \"{0}_relaxed.pdb\".format(protein)))\n",
    "\n",
    "            # Process FoldSeek results for AF50m database\n",
    "            data_af50m = pd.read_table(os.path.join(batch_dir, \"{0}_relaxed_aln_af50m.txt\".format(protein)), header=None, names=[\"query\",\"target\",\"fident\",\"alnlen\",\"mismatch\",\"gapopen\",\"qstart\",\"qend\",\"tstart\",\"tend\",\"evalue\",\"bits\",\"prob\",\"lddt\",\"lddtfull\"])\n",
    "\n",
    "        for i in range(0,len(data_af50m)):\n",
    "            data_af50m.at[i,\"pLDDT_qAln\"] = get_pLDDT(structure, data_af50m.at[i,\"qstart\"],data_af50m.at[i,\"qend\"])\n",
    "      \n",
    "        data_af50m_filtered = data_af50m\n",
    "        if len(data_af50m_filtered) != 0:\n",
    "            data_af50m_filtered = data_af50m_filtered[(data_af50m_filtered[\"prob\"] > 0.5) & (data_af50m_filtered[\"evalue\"] < 1e-3) & (data_af50m_filtered[\"lddt\"] >= 0.5) & (data_af50m_filtered[\"pLDDT_qAln\"] >= 0.7)]\n",
    "        data_af50m_filtered = data_af50m_filtered.reset_index()\n",
    "       \n",
    "        data_af50m_filtered.to_csv(os.path.join(batch_dir, f\"{protein}_foldseek_af50m_filtered.csv\"), index=False)     \n",
    "     \n",
    "            # Process FoldSeek results for PDB database\n",
    "        \n",
    "        data_pdb = pd.read_table(os.path.join(batch_dir, \"{0}_relaxed_aln_pdb.txt\".format(protein)), header=None, names=[\"query\",\"target\",\"fident\",\"alnlen\",\"mismatch\",\"gapopen\",\"qstart\",\"qend\",\"tstart\",\"tend\",\"evalue\",\"bits\",\"prob\",\"lddt\",\"lddtfull\"])\n",
    "\n",
    "        for i in range(0,len(data_pdb)):\n",
    "            data_pdb.at[i,\"pLDDT_qAln\"] = get_pLDDT(structure, data_pdb.at[i,\"qstart\"],data_pdb.at[i,\"qend\"])\n",
    "         \n",
    "        data_pdb_filtered = data_pdb\n",
    "       \n",
    "        if len(data_pdb_filtered) != 0:\n",
    "            data_pdb_filtered = data_pdb_filtered[(data_pdb_filtered[\"prob\"] > 0.5) & (data_pdb_filtered[\"evalue\"] < 1e-3) & (data_pdb_filtered[\"lddt\"] >= 0.5) & (data_pdb_filtered[\"pLDDT_qAln\"] >= 0.7)]\n",
    "        data_pdb_filtered = data_pdb_filtered.reset_index()\n",
    "                # store filtered FoldSeek results\n",
    "        data_pdb_filtered.to_csv(os.path.join(batch_dir, f\"{protein}_foldseek_pdb_filtered.csv\"), index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f03960a-a194-42c9-a82f-dcdf87495c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now adding for treating phold - phrogs foldseek results\n",
    "pipeline_search_dir = \"c_structure_annotation/structure_comp\"  \n",
    "\n",
    "for index_batch in range(1, 6):  \n",
    "    batch_dir = os.path.join(pipeline_search_dir, f\"batch_{index_batch}\")\n",
    "\n",
    "    for filename in os.listdir(batch_dir):\n",
    "        if filename.endswith(\"_relaxed.pdb\"): \n",
    "            protein = filename.replace(\"_relaxed.pdb\", \"\")  \n",
    "\n",
    "            structure = p.get_structure(protein, os.path.join(batch_dir, \"{0}_relaxed.pdb\".format(protein)))\n",
    "\n",
    "            # Process FoldSeek results for Phrogs database\n",
    "            data_phold = pd.read_table(os.path.join(batch_dir, \"{0}_relaxed_phold.txt\".format(protein)), header=None, names=[\"query\",\"target\",\"fident\",\"alnlen\",\"mismatch\",\"gapopen\",\"qstart\",\"qend\",\"tstart\",\"tend\",\"evalue\",\"bits\",\"prob\",\"lddt\",\"lddtfull\"])\n",
    "\n",
    "        for i in range(0,len(data_phold)):\n",
    "            data_phold.at[i,\"pLDDT_qAln\"] = get_pLDDT(structure, data_phold.at[i,\"qstart\"],data_phold.at[i,\"qend\"])\n",
    "      \n",
    "        data_phold_filtered = data_phold\n",
    "        if len(data_phold_filtered) != 0:\n",
    "            ddata_phold_filtered = data_phold_filtered[(data_phold_filtered[\"prob\"] > 0.5) & (data_phold_filtered[\"evalue\"] < 1e-3) & (data_phold_filtered[\"lddt\"] >= 0.5) & (data_phold_filtered[\"pLDDT_qAln\"] >= 0.7)]\n",
    "        data_phold_filtered = data_phold_filtered.reset_index()\n",
    "       \n",
    "        data_phold_filtered.to_csv(os.path.join(batch_dir, f\"{protein}_foldseek_phold_filtered.csv\"), index=False)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "663d7972-9eb1-4945-83e9-f3c242012600",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_search_dir = \"c_structure_annotation\"  \n",
    "\n",
    "for index_batch in range(1, 6):  \n",
    "    batch_dir = os.path.join(pipeline_search_dir, \"structure_comp\", f\"batch_{index_batch}\")\n",
    "\n",
    "    # Check all proteins in the batches\n",
    "    for filename in os.listdir(batch_dir):\n",
    "        if filename.endswith(\"_relaxed.pdb\"):  \n",
    "            protein = filename.replace(\"_relaxed.pdb\", \"\")  \n",
    "\n",
    "            csv_af50m = os.path.join(batch_dir, f\"{protein}_foldseek_af50m_filtered.csv\")\n",
    "            csv_pdb = os.path.join(batch_dir, f\"{protein}_foldseek_pdb_filtered.csv\")\n",
    "            csv_phold = os.path.join(batch_dir, f\"{protein}_foldseek_phold_filtered.csv\")\n",
    "\n",
    "            # Check if all files exist\n",
    "            if not (os.path.isfile(csv_af50m) and os.path.isfile(csv_pdb)) and (os.path.isfile(csv_phold)) :\n",
    "                print(f\"Issue finding FoldSeek filtered output files for protein {protein} in batch_{index_batch}.\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65dda02e-6628-4512-adc6-2bd2f70d9309",
   "metadata": {},
   "source": [
    "No error message -  all csv files created successfully for each protein in every batch, now moving on to analysing results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "072cd07e-eae8-4beb-99ed-63a2c0983bbe",
   "metadata": {},
   "source": [
    "#### Analysing outputs - Extracting the UniProt IDs from the filtered FoldSeek output files\n",
    "Next, we will extract the UniProt IDs from the filtered FoldSeek results. For the searches against the AlphaFold database, this is quite straightforward (the UniProt ID can be extracted directly from the target column), but for the searched against the PDB database, this requires linking PDB entries to UniProt IDs. After extraction, the unique identifiers are stored in dictionaries, linking the UniProt ID to the protein for which a structure model of this UniProt ID was a hit. For the PDB entries, we also store the connection between UniProt ID and PDB ID with chain in a seperate dictionary. For reproducibility purposes, we will store these dictionaries and the list of obsolete PDB identifiers on the date of search to .csv files, as database updates could make this step of the pipeline non reproducible.First, we'll deal with the output files from the search against the AlphaFold database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f1c8721-26ba-47d2-ba5b-9b3c1578f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract UniProt ID from the FoldSeek target column\n",
    "def get_UniProtID_from_target(target):\n",
    "    uniprot_id = target.split('-')[1]\n",
    "    return uniprot_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "970806d5-d5e7-4948-bdee-223e3f617675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate empty dictionary\n",
    "dict_af50m_UniProtID_NCBIid = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a39bf7cc-e0df-4692-9490-24a604e951fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through batches, then through proteins\n",
    "\n",
    "pipeline_search_dir = \"c_structure_annotation/structure_comp\"  \n",
    "for index_batch in range(1, 6): \n",
    "    batch_dir = os.path.join(pipeline_search_dir, f\"batch_{index_batch}\")\n",
    "    protein_files = [f for f in os.listdir(batch_dir) if f.endswith('_foldseek_af50m_filtered.csv')]\n",
    "    \n",
    "    for file_name in protein_files:\n",
    "        protein = file_name.replace('_foldseek_af50m_filtered.csv', '')\n",
    "   \n",
    "        filtered_foldseek = pd.read_csv(os.path.join(batch_dir, f\"{protein}_foldseek_af50m_filtered.csv\"))\n",
    "        if not filtered_foldseek.empty:\n",
    "            for i in range(0,len(filtered_foldseek)):\n",
    "                # extract UniProt ID\n",
    "                uniprot_id = get_UniProtID_from_target(filtered_foldseek.at[i, \"target\"])\n",
    "                # if UniProt ID already in dictionary: add NCBI ID to list of already matched NCBI IDs as value for the UniProt ID\n",
    "                if uniprot_id in dict_af50m_UniProtID_NCBIid.keys():\n",
    "                    prot_list = dict_af50m_UniProtID_NCBIid.get(uniprot_id)\n",
    "                # if UniProt ID not yet in dictionary: create new key-value pair linking NCBI ID to UniProt ID\n",
    "                if uniprot_id not in dict_af50m_UniProtID_NCBIid.keys():\n",
    "                    prot_list = list()\n",
    "                prot_list.append(protein)\n",
    "                dict_af50m_UniProtID_NCBIid[uniprot_id] = list(set(prot_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c14d234c-6ef6-4929-b1e1-76517d1a968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # store dict to file\n",
    "    # here, this is not needed for reproducibility, but better to be consistent + saves time when rerunning\n",
    "with open(os.path.join(pipeline_search_dir, \"dict_af50m_UniProtID_NCBIid.csv\"), \"w\", newline=\"\") as fp:\n",
    "    # create a writer object\n",
    "    writer = csv.DictWriter(fp, fieldnames = dict_af50m_UniProtID_NCBIid.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(dict_af50m_UniProtID_NCBIid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d3716-5c6e-4cbd-824a-ecfa0bd727ee",
   "metadata": {},
   "source": [
    "we have the alphafold results\n",
    "now moving on to PDB results: output files against PDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e16458d1-353c-4763-b8e5-142b7fe0cac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of obsolete PDB IDs on date of search (Feb 8th, 2025)\n",
    "obsolete_ids = Bio.PDB.PDBList.get_all_obsolete(Bio.PDB.PDBList())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50eec30d-c9c6-41a7-aab6-8334ae8f840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store list of obsolete PDB IDs to file\n",
    "with open(os.path.join(pipeline_search_dir, \"list_obsolete_PDB_ids.txt\"), \"w+\") as file:\n",
    "    for pdb_id in obsolete_ids:\n",
    "        file.write(f\"{pdb_id} \\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6053650c-0e35-4440-951d-1d8c9c0952a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary functions to ...\n",
    "\n",
    "# from FoldSeek target column, extract PDB id and chain\n",
    "def get_PDBid_chain_from_target(target):\n",
    "    pdb_id = target.split('.')[0]\n",
    "    chain = target.split('_')[-1]\n",
    "    return pdb_id, chain\n",
    "\n",
    "# check if PDB ID is obsolete\n",
    "def check_PDBid_active(pdb_id, obsolete_list):\n",
    "    if pdb_id.upper() in obsolete_list:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# from PDB ID, get all PDB polymer entities\n",
    "def get_PDB_entities_from_entry(pdb_id):\n",
    "    url = f\"https://data.rcsb.org/rest/v1/core/entry/{pdb_id}\"\n",
    "    r = sess.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data[\"rcsb_entry_container_identifiers\"].get(\"polymer_entity_ids\")\n",
    "\n",
    "# match PDB entity to protein chain\n",
    "def get_PDB_entity_with_chain(entity_ids,pdb_id,chain):\n",
    "    for entity in entity_ids:\n",
    "        url = f\"https://data.rcsb.org/rest/v1/core/polymer_entity/{pdb_id}/{entity}\"\n",
    "        r = sess.get(url)\n",
    "        data = json.loads(r.text)\n",
    "        # assumption: FoldSeek gives author chain naming, not PDB renamed chain names\n",
    "        if chain in data[\"rcsb_polymer_entity_container_identifiers\"].get(\"auth_asym_ids\"):\n",
    "            return entity\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# get UniProt ID based on the PDB ID and entity (corresponding to the chain reported by FoldSeek)\n",
    "def get_UniProt_from_PDB_entity(pdb_id,entity):\n",
    "    url = f\"https://data.rcsb.org/rest/v1/core/polymer_entity/{pdb_id}/{entity}\"\n",
    "    r = sess.get(url)\n",
    "    if not r.ok:\n",
    "      r.raise_for_status()\n",
    "    data = json.loads(r.text)\n",
    "    return data[\"rcsb_polymer_entity_container_identifiers\"].get(\"uniprot_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ddae30d-c4f4-4605-8d0e-7696e2cf1b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate empty dictionaries\n",
    "dict_pdb_UniProtID_NCBIid = {}\n",
    "dict_pdb_PDBidch_UniProtID = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c79aeb38-5553-4e2b-a51a-d24595e13f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_search_dir = \"c_structure_annotation/structure_comp\"  \n",
    "# loop through batches, then through proteins\n",
    "for index_batch in range(1, 6): \n",
    "    batch_dir = os.path.join(pipeline_search_dir, f\"batch_{index_batch}\")\n",
    "    protein_files = [f for f in os.listdir(batch_dir) if f.endswith('_foldseek_pdb_filtered.csv')]\n",
    "\n",
    "    for file_name in protein_files:\n",
    "        protein = file_name.replace('_foldseek_pdb_filtered.csv', '')\n",
    "  \n",
    "        filtered_foldseek = pd.read_csv(os.path.join(batch_dir, f\"{protein}_foldseek_pdb_filtered.csv\"))\n",
    "        # check if it has any content\n",
    "        if not filtered_foldseek.empty:\n",
    "            # loop over entries\n",
    "            for i in range(0,len(filtered_foldseek)):\n",
    "                # extract PDB ID and chain\n",
    "                pdb_id, chain = get_PDBid_chain_from_target(filtered_foldseek.at[i,\"target\"])\n",
    "                # if PDB ID chain combo already in dictionary\n",
    "                if f\"{pdb_id}_{chain}\" in dict_pdb_PDBidch_UniProtID.keys():\n",
    "                    # get UniProt ID previously stored\n",
    "                    uniprot_ids = dict_pdb_PDBidch_UniProtID.get(f\"{pdb_id}_{chain}\")\n",
    "                    if uniprot_ids is not None:\n",
    "                        for uniprot_id in uniprot_ids:\n",
    "                            # add NCBI ID to list of matched UniProt IDs\n",
    "                            if uniprot_id not in dict_pdb_UniProtID_NCBIid.keys():\n",
    "                                prot_list = list()\n",
    "                            if uniprot_id in dict_pdb_UniProtID_NCBIid.keys():\n",
    "                                prot_list = dict_pdb_UniProtID_NCBIid.get(uniprot_id)\n",
    "                            prot_list.append(protein)\n",
    "                            dict_pdb_UniProtID_NCBIid[uniprot_id] = list(set(prot_list))\n",
    "                    \n",
    "                # if PDB ID chain combo not yet in dictionary\n",
    "                if f\"{pdb_id}_{chain}\" not in dict_pdb_PDBidch_UniProtID.keys():\n",
    "                    # check if PDB ID is obsolete\n",
    "                    if not check_PDBid_active(pdb_id, obsolete_ids):\n",
    "                        # if obsolete, instead of storing a UniProt ID we store \"Obsolete PDB ID\"\n",
    "                        dict_pdb_PDBidch_UniProtID[f\"{pdb_id}_{chain}\"] = \"Obsolete PDB ID\"\n",
    "                    if check_PDBid_active(pdb_id, obsolete_ids):\n",
    "                        # fetch UniProt ID(s) - this can be multiple\n",
    "                        uniprot_ids = get_UniProt_from_PDB_entity(pdb_id,get_PDB_entity_with_chain(get_PDB_entities_from_entry(pdb_id),pdb_id,chain))\n",
    "                        # add link PDB id chain combo to UniProt IDs\n",
    "                        dict_pdb_PDBidch_UniProtID[f\"{pdb_id}_{chain}\"] = uniprot_ids\n",
    "                        if uniprot_ids is not None:\n",
    "                        # link UniProt IDs to NCBI protein ids\n",
    "                            for uniprot_id in uniprot_ids:\n",
    "                                if uniprot_id not in dict_pdb_UniProtID_NCBIid.keys():\n",
    "                                    prot_list = list()\n",
    "                                if uniprot_id in dict_pdb_UniProtID_NCBIid.keys():\n",
    "                                    prot_list = dict_pdb_UniProtID_NCBIid.get(uniprot_id)\n",
    "                                prot_list.append(protein)\n",
    "                                dict_pdb_UniProtID_NCBIid[uniprot_id] = list(set(prot_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a5820e3-71ef-4716-8c38-50dc0a19957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # store dictionaries to file\n",
    "    # dictionary storing link UniProt ID - NCBI ID\n",
    "with open(os.path.join(pipeline_search_dir, \"dict_pdb_UniProtID_NCBIid.csv\"), \"w\", newline=\"\") as fp:\n",
    "    writer = csv.DictWriter(fp, fieldnames = dict_pdb_UniProtID_NCBIid.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(dict_pdb_UniProtID_NCBIid)\n",
    "    # dictionary storing link PDB entry (ID and chain) - UniProt ID\n",
    "with open(os.path.join(pipeline_search_dir, \"dict_pdb_PDBidch_UniProtID.csv\"), \"w\", newline=\"\") as fp:\n",
    "    writer = csv.DictWriter(fp, fieldnames = dict_pdb_PDBidch_UniProtID.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(dict_pdb_PDBidch_UniProtID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add9ee7e-e6aa-4b09-bdc0-922343031b61",
   "metadata": {},
   "source": [
    "Step 2 - Obtaining the UniProt function description based on the UniProt IDs\n",
    "Next, we will extract the function description for all proteins for which we obtained the UniProt IDs from the filtered FoldSeek results. Executed July 9-10 Feb, 2025, UniProt release 2025_01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8831916d-4f3d-4c5f-801c-ec6591242a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to ...\n",
    "\n",
    "# get protein name, for a single UniProt ID\n",
    "def get_protein_name_string(uniprot_id):\n",
    "    # note: unsure if protein_name is a required field, if we get errors, look into this!\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/search?query={uniprot_id}&fields=protein_name&format=tsv\"\n",
    "    r = sess.get(url)\n",
    "    r.raise_for_status()\n",
    "    content = r.text\n",
    "    names = content.split('\\n')[1:-1]\n",
    "    # if the UniProt entry was marked as obsolete, access its UniParc accession\n",
    "    if \"deleted\" in names:\n",
    "        try:\n",
    "            return \";\".join([str(name) for name in get_uniparc_entry(uniprot_id)])\n",
    "        except TypeError:\n",
    "            return \"WARNING UniProt ID is obsolete and issue with fetching UniParc information\"\n",
    "    else:\n",
    "        return \";\".join([str(name) for name in names])\n",
    "\n",
    "# get protein names (and whether the protein is still in UniProt) from a UniParc entry matching the obsolete UniProt ID\n",
    "def get_uniparc_entry(uniprot_id):\n",
    "    requestURL = f\"https://www.ebi.ac.uk/proteins/api/uniparc/accession/{uniprot_id}\"\n",
    "    r = requests.get(requestURL, headers={ \"Accept\" : \"application/json\"})\n",
    "    if not r.ok:\n",
    "      r.raise_for_status()\n",
    "    responseBody = r.text\n",
    "    data = json.loads(r.text)\n",
    "    uniparc_entries = []\n",
    "    uniparc_entries.append(\"WARNING UniProt ID obsolete, info fetched from UniParc\")\n",
    "    for i in range(0,len(data[\"dbReference\"])):\n",
    "        for j in data[\"dbReference\"][i].get(\"property\"):\n",
    "            if j.get(\"type\") == \"protein_name\":\n",
    "                uniparc_entries.append(f\"{data['dbReference'][i].get('id')} : {j.get('value')} (active UniProt ID: {data['dbReference'][i].get('active')})\")\n",
    "    return uniparc_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef8502d4-a2fe-4e1b-86e0-aaecd7c5e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of failed IDs\n",
    "failed_UniProtID_function = list()\n",
    "# dict storing functions\n",
    "dict_UniProtID_function = {}\n",
    "# get all the UniProt IDs from the two dictionaries:\n",
    "uniprot_to_search = list(set(list(dict_af50m_UniProtID_NCBIid.keys()) + list(dict_pdb_UniProtID_NCBIid.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc8f3b-9107-4aa2-bb26-958a36ac0a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search each element, store in dict\n",
    "for uniprot_id in uniprot_to_search:\n",
    "    # this loop allows us to rerun upon errors\n",
    "    if uniprot_id in dict_UniProtID_function.keys():\n",
    "        continue\n",
    "    if uniprot_id not in dict_UniProtID_function.keys():\n",
    "        try:\n",
    "            function = get_protein_name_string(uniprot_id)\n",
    "            dict_UniProtID_function[uniprot_id] = function\n",
    "        except requests.exceptions.ReadTimeout:\n",
    "            failed_UniProtID_function.append(uniprot_id)\n",
    "            print(f\"Failed to fetch information on UniProt ID {uniprot_id} due to ReadTimeout error. Stored this ID to the list of failed IDs. Try again later.\")\n",
    "        except JSONDecodeError:\n",
    "            failed_UniProtID_function.append(uniprot_id)\n",
    "            print(f\"Failed to fetch information on UniProt ID {uniprot_id} due to JSON decode error. Stored this ID to the list of failed IDs. Try again later.\")\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            failed_UniProtID_function.append(uniprot_id)\n",
    "            print(f\"Failed to fetch information on UniProt ID {uniprot_id} due to Connection error. Stored this ID to the list of failed IDs. Try again later.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18426a7c-dcbf-4815-9a3c-8d6903f8826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the dictionary to file\n",
    "with open(os.path.join(pipeline_search_dir, \"dict_UniProtID_function.csv\"), \"w\", newline=\"\") as fp:\n",
    "    # Create a writer object\n",
    "    writer = csv.DictWriter(fp, fieldnames=dict_UniProtID_function.keys())\n",
    "    # Write the data rows\n",
    "    writer.writeheader()\n",
    "    writer.writerow(dict_UniProtID_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fe2d69-fd1c-45b0-b021-38d0b43c05fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_UniProtID_function = {}\n",
    "failed_UniProtID_function = []\n",
    "\n",
    "for uniprot_id in uniprot_to_search:\n",
    "    if uniprot_id in dict_UniProtID_function.keys():\n",
    "        continue\n",
    "    try:\n",
    "        function = get_protein_name_string(uniprot_id)\n",
    "        dict_UniProtID_function[uniprot_id] = function\n",
    "    except requests.exceptions.ReadTimeout:\n",
    "        failed_UniProtID_function.append(uniprot_id)\n",
    "        print(f\"Failed to fetch information on UniProt ID {uniprot_id} due to ReadTimeout error.\")\n",
    "    except JSONDecodeError:\n",
    "        failed_UniProtID_function.append(uniprot_id)\n",
    "        print(f\"Failed to fetch information on UniProt ID {uniprot_id} due to JSON decode error.\")\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        failed_UniProtID_function.append(uniprot_id)\n",
    "        print(f\"Failed to fetch information on UniProt ID {uniprot_id} due to Connection error.\")\n",
    "\n",
    "    # Write current state to CSV after each iteration\n",
    "    with open(os.path.join(pipeline_search_dir, \"dict_UniProtID_function.csv\"), \"w\", newline=\"\") as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        writer.writerow([\"UniProtID\", \"Function\"])\n",
    "        for key, value in dict_UniProtID_function.items():\n",
    "            writer.writerow([key, value])\n",
    "\n",
    "print(\"Final dictionary saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0d1501b-297c-4c34-a33a-9785a332b555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary updated and saved.\n"
     ]
    }
   ],
   "source": [
    "#to run if above cell was not completed in one session\n",
    "import csv\n",
    "import os\n",
    "import requests\n",
    "from json import JSONDecodeError\n",
    "\n",
    "csv.field_size_limit(10 * 1024 * 1024)  # Increase limit to 10MB\n",
    "\n",
    "pipeline_search_dir = \"c_structure_annotation/structure_comp\"  \n",
    "csv_path = os.path.join(pipeline_search_dir, \"dict_UniProtID_function.csv\")\n",
    "\n",
    "dict_UniProtID_function = {}\n",
    "failed_UniProtID_function = []\n",
    "\n",
    "# Load existing data from CSV if it exists\n",
    "if os.path.exists(csv_path):\n",
    "    with open(csv_path, newline='') as fp:\n",
    "        reader = csv.reader(fp)\n",
    "        next(reader)  # Skip header\n",
    "        for row in reader:\n",
    "            if row:\n",
    "                dict_UniProtID_function[row[0]] = row[1]\n",
    "\n",
    "for uniprot_id in uniprot_to_search:\n",
    "    if uniprot_id in dict_UniProtID_function:\n",
    "        continue  # Skip if already processed\n",
    "\n",
    "    try:\n",
    "        function = get_protein_name_string(uniprot_id)\n",
    "        dict_UniProtID_function[uniprot_id] = function\n",
    "    except requests.exceptions.ReadTimeout:\n",
    "        failed_UniProtID_function.append(uniprot_id)\n",
    "        print(f\"Failed to fetch {uniprot_id}: ReadTimeout error.\")\n",
    "    except JSONDecodeError:\n",
    "        failed_UniProtID_function.append(uniprot_id)\n",
    "        print(f\"Failed to fetch {uniprot_id}: JSON decode error.\")\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        failed_UniProtID_function.append(uniprot_id)\n",
    "        print(f\"Failed to fetch {uniprot_id}: Connection error.\")\n",
    "\n",
    "    # Incrementally save to CSV after each processed item\n",
    "    with open(csv_path, \"w\", newline=\"\") as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        writer.writerow([\"UniProtID\", \"Function\"])\n",
    "        for key, value in dict_UniProtID_function.items():\n",
    "            writer.writerow([key, value])\n",
    "\n",
    "print(\"Dictionary updated and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eaaf20-5b60-4e07-b230-0bd44f547131",
   "metadata": {},
   "source": [
    " Step 3: Writing output files with function annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "faf02dbc-bf1f-44be-9bf5-1c1b10dde3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate lists\n",
    "    # one with all the UniProt IDs for which the function description contains 'acetyltransferase' or 'GNAT'\n",
    "uniprot_ids_to_annotate = list()\n",
    "    # one with all the NCBI IDs linked to these UniProt IDs \n",
    "    # (i.e., proteins for which at least one of the filtered FoldSeek hits was annotated as 'acetyltransferase' or 'GNAT')\n",
    "ncbi_ids_to_annotate = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "965e496f-7dc6-464c-a1c6-5f26369c98fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over function dict\n",
    "for key, value in dict_UniProtID_function.items():\n",
    "    # get UniProt IDs related to function of interest\n",
    "    uniprot_ids_to_annotate.append(key)\n",
    "\n",
    "\n",
    "# loop over list of UniProt IDs with function of interest\n",
    "for uniprot_id in list(set(uniprot_ids_to_annotate)):\n",
    "    # get NCBI ids for proteins for which we want to annotate the FoldSeek results fully\n",
    "    ncbi_ids_to_annotate.append(dict_pdb_UniProtID_NCBIid.get(uniprot_id))\n",
    "    ncbi_ids_to_annotate.append(dict_af50m_UniProtID_NCBIid.get(uniprot_id))\n",
    "\n",
    "ncbi_ids_to_annotate = [i for i in list(set(collapse(ncbi_ids_to_annotate))) if i is not None]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9febb19-09ea-4000-bb59-961f569e5f89",
   "metadata": {},
   "outputs": [],
   "source": [
    " # store this list of NCBI ids to file\n",
    "with open(os.path.join(pipeline_search_dir, \"list_ncbiid_possibleact.txt\"), \"w+\") as file:\n",
    "    for ncbi_id in ncbi_ids_to_annotate:\n",
    "        file.write(f\"{ncbi_id}\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12b3a649-4a28-4a2c-8621-266b6cb6b79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_search_dir = \"c_structure_annotation/structure_comp\"  \n",
    "csv_path = os.path.join(pipeline_search_dir, \"dict_pdb_PDBidch_UniProtID.csv\")\n",
    "\n",
    "with open(csv_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "keys = lines[0].strip().split(',')\n",
    "values = lines[1].strip().split(',')\n",
    "dict_pdb_PDBidch_UniProtID = dict(zip(keys, values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcb84030-9489-4ee3-b4de-2692474c6208",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_search_dir = \"c_structure_annotation/structure_comp\"  \n",
    "csv_path = os.path.join(pipeline_search_dir, \"dict_UniProtID_function.csv\")\n",
    "dict_UniProtID_function = pd.read_csv(csv_path).set_index('UniProtID')['Function'].to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40283544-19da-4b68-bc08-4ef0896c6726",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(pipeline_search_dir, \"list_ncbiid_possibleact.txt\"), \"r\") as file:\n",
    "    ncbi_ids_to_annotate = [line.strip() for line in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "284cfb1b-55c0-485a-9f2c-7729927e1519",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_batch in range(1, 6): \n",
    "    batch_dir = os.path.join(pipeline_search_dir, f\"batch_{index_batch}\")\n",
    "    protein_files = [f for f in os.listdir(batch_dir) if f.endswith('_foldseek_af50m_filtered.csv')]\n",
    "\n",
    "    for file_name in protein_files:\n",
    "        protein = file_name.replace('_foldseek_af50m_filtered.csv', '')\n",
    "\n",
    "        data_af50m_filtered = pd.read_csv(os.path.join(batch_dir, f\"{protein}_foldseek_af50m_filtered.csv\")) \n",
    "        # Check if it has any content\n",
    "        if len(data_af50m_filtered) != 0:\n",
    "            for i in range(len(data_af50m_filtered)):\n",
    "                if protein in ncbi_ids_to_annotate:\n",
    "                    data_af50m_filtered.at[i, \"protein_descr\"] = dict_UniProtID_function.get(get_UniProtID_from_target(data_af50m_filtered.at[i, \"target\"]))\n",
    "            # Store results\n",
    "            data_af50m_filtered.to_csv(os.path.join(batch_dir, f\"{protein}_foldseek_af50m_processed.csv\"), index=False)     \n",
    "\n",
    "        # PDB hits\n",
    "        data_pdb_filtered = pd.read_csv(os.path.join(batch_dir, f\"{protein}_foldseek_pdb_filtered.csv\")) \n",
    "        if len(data_pdb_filtered) != 0:\n",
    "            for i in range(len(data_pdb_filtered)):\n",
    "                pdb_id, chain = get_PDBid_chain_from_target(data_pdb_filtered.at[i, \"target\"])\n",
    "                # Get UniProt ID or obsolete from dictionary\n",
    "                uniprot_ids = dict_pdb_PDBidch_UniProtID.get(f\"{pdb_id}_{chain}\")\n",
    "                if uniprot_ids == \"Obsolete PDB ID\":\n",
    "                    data_pdb_filtered.at[i, \"protein_descr\"] = \"WARNING This PDB ID is obsolete, hence, no information could be fetched\"\n",
    "                elif uniprot_ids is None:\n",
    "                    data_pdb_filtered.at[i, \"protein_descr\"] = \"WARNING No UniProt ID was linked to this PDB entry, hence, no information could be fetched\"\n",
    "                else:\n",
    "                    string = \"\"\n",
    "                    for uniprot_id in uniprot_ids.split(';'):  # Assuming multiple UniProt IDs are separated by semicolons\n",
    "                        string += dict_UniProtID_function.get(uniprot_id, 'Unknown Function') + f\" [UniProt ID: {uniprot_id}] \"\n",
    "                    data_pdb_filtered.at[i, \"protein_descr\"] = string.strip()\n",
    "            # Store results\n",
    "            data_pdb_filtered.to_csv(os.path.join(batch_dir, f\"{protein}_foldseek_pdb_processed.csv\"), index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d028c609",
   "metadata": {},
   "source": [
    "we have the alphafold and PDB results\n",
    "now moving on to phold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3f4a182-dbcf-43ed-8107-c843f8f59afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning function to remove extra details in query and target col so easier matching with known_annot_phrogs \n",
    "def clean_foldseek_ids(query, target):\n",
    "    clean_query = re.sub(r'_relaxed$', '', query)\n",
    "    clean_target = re.sub(r'^envhog_', '', target)\n",
    "    clean_target = re.sub(r'protein', '', clean_target)\n",
    "    clean_target = re.sub(r'\\.\\w+$', '', clean_target) \n",
    "    \n",
    "    return clean_query, clean_target\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "450fd381-902c-4cc8-a32c-0520a5b891c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_search_dir = \"c_structure_annotation/structure_comp\"  \n",
    "csv_path = os.path.join(pipeline_search_dir, \"known_annot_phrogs.tsv\")\n",
    "known_annot = pd.read_csv(csv_path, sep='\\t')\n",
    "\n",
    "# Merge \"query\" and \"target\" columns into a new column \"query:target\"\n",
    "known_annot[\"query:target\"] = known_annot[\"query\"].astype(str) + \":\" + known_annot[\"target\"].astype(str)\n",
    "\n",
    "# Keep only the new column along with \"annot\" and \"category\"\n",
    "known_annot = known_annot[[\"query:target\", \"annot\", \"category\"]]\n",
    "\n",
    "# Save the updated file\n",
    "updated_file_path = \"phrogs_annot.tsv\"  \n",
    "known_annot.to_csv(updated_file_path, sep='\\t', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612cc418-be39-4a8b-8519-97e9dcadb40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_foldseek_ids(query, target):\n",
    "    clean_query = re.sub(r'_relaxed$', '', query)\n",
    "    clean_target = re.sub(r'^envhog_', '', target)\n",
    "    clean_target = re.sub(r'protein', '', clean_target)\n",
    "    clean_target = re.sub(r'\\.\\w+$', '', clean_target)\n",
    "    return clean_query, clean_target\n",
    "\n",
    "def extract_phrog_number(target):\n",
    "    match = re.search(r'phrog_(\\d+)', target)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "\n",
    "pipeline_search_dir = \"c_structure_annotation/structure_comp\"  \n",
    "csv_path = os.path.join(pipeline_search_dir, \"known_annot_phrogs.tsv\")\n",
    "known_annot = pd.read_csv(csv_path, sep='\\t')\n",
    "known_annot['phrog_number'] = known_annot['query'].apply(lambda x: re.search(r'phrog_(\\d+)', x).group(1) if re.search(r'phrog_(\\d+)', x) else None)\n",
    "\n",
    "\n",
    "for index_batch in range(1, 5):\n",
    "    batch_dir = os.path.join(pipeline_search_dir, f\"batch_{index_batch}\")\n",
    "    \n",
    "    protein_files = [f for f in os.listdir(batch_dir) if f.endswith('_foldseek_phold_filtered.csv')]\n",
    "\n",
    "    for file_name in protein_files:\n",
    "        protein = file_name.replace('_foldseek_phold_filtered.csv', '')\n",
    "        data_phold_filtered = pd.read_csv(os.path.join(batch_dir, file_name))\n",
    "        data_phold_filtered['phrog_number'] = data_phold_filtered['target'].apply(extract_phrog_number)\n",
    "\n",
    "        results = []\n",
    "        for _, row in data_phold_filtered.iterrows():\n",
    "            phrog_num = row['phrog_number']\n",
    "            match = known_annot[known_annot['phrog_number'] == phrog_num]\n",
    "            annotation = match['annot'].values[0] if not match.empty else 'Uncharacterized Protein'\n",
    "            results.append(annotation)\n",
    "\n",
    "        data_phold_filtered['annotation'] = results\n",
    "        data_phold_filtered.to_csv(os.path.join(batch_dir, f\"{protein}_foldseek_phold_processed.csv\"), index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
